{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb86vqP_Y2Ep"
      },
      "outputs": [],
      "source": [
        "# Make sure that you have all these libaries available to run the code successfully\n",
        "from pandas_datareader import data\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import urllib.request, json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense, Conv1D, MaxPooling1D\n",
        "import yfinance as yf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e310ocOaZD1A",
        "outputId": "99aa0712-fe1f-44b3-fd7f-c938b6c1bb0e"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade alpaca-py numpy statsmodels arch scikit-learn pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jwuqXqLnY9JJ",
        "outputId": "658d7329-bb55-4eb0-f9bb-75dc379feff8"
      },
      "outputs": [],
      "source": [
        "from alpaca.data.historical import StockHistoricalDataClient\n",
        "from alpaca.data.requests   import StockBarsRequest\n",
        "from alpaca.data.timeframe  import TimeFrame, TimeFrameUnit\n",
        "import datetime as dt\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "KEY = \"PKCLL4TXCDLRN76OGRAB\"\n",
        "SECRET = \"ig5CGnl3c1jXEepU6VK5DPXgsV5WSOBYrIJGk70T\"\n",
        "\n",
        "def _parse_timeframe(freq: str) -> TimeFrame:\n",
        "    \"\"\"\n",
        "    Convert strings like '5Min', '12H', '1D', '1W', '3M' to an Alpaca TimeFrame.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ValueError if the format is unrecognised.\n",
        "    \"\"\"\n",
        "    match = re.fullmatch(r\"(\\d+)([A-Za-z]+)\", freq)\n",
        "    if not match:\n",
        "        raise ValueError(f\"Bad timeframe '{freq}'. Try '5Min', '12H', '1D', etc.\")\n",
        "    n, unit = int(match.group(1)), match.group(2).lower()\n",
        "\n",
        "    if unit in (\"min\", \"t\"):\n",
        "        return TimeFrame(n, TimeFrameUnit.Minute)\n",
        "    if unit in (\"hour\", \"h\"):\n",
        "        return TimeFrame(n, TimeFrameUnit.Hour)\n",
        "    if unit in (\"day\", \"d\"):\n",
        "        return TimeFrame(n, TimeFrameUnit.Day)\n",
        "    if unit in (\"week\", \"w\"):\n",
        "        return TimeFrame(n, TimeFrameUnit.Week)\n",
        "    if unit in (\"month\", \"m\"):\n",
        "        return TimeFrame(n, TimeFrameUnit.Month)\n",
        "\n",
        "    raise ValueError(f\"Unsupported timeframe unit '{unit}' in '{freq}'\")\n",
        "\n",
        "\n",
        "def get_bars_df(\n",
        "        ticker: str,\n",
        "        start_date: str,\n",
        "        end_date: str,\n",
        "        timeframe: str,\n",
        "        api_key: str,\n",
        "        secret_key: str,\n",
        "        tz: dt.tzinfo = dt.timezone.utc\n",
        "    ) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download historical bars and return a tidy DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ticker      : e.g. 'SPY'\n",
        "    start_date  : 'YYYY-MM-DD'\n",
        "    end_date    : 'YYYY-MM-DD'\n",
        "    timeframe   : '5Min', '12H', '1D', '1W', '3M', …\n",
        "    api_key     : Alpaca API key\n",
        "    secret_key  : Alpaca secret key\n",
        "    tz          : timezone for returned index (default UTC)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pandas.DataFrame with a (tz-aware) DatetimeIndex and standard OHLCV columns.\n",
        "    \"\"\"\n",
        "    tf = _parse_timeframe(timeframe)\n",
        "\n",
        "    client = StockHistoricalDataClient(api_key, secret_key)\n",
        "\n",
        "    req = StockBarsRequest(\n",
        "        symbol_or_symbols=ticker,\n",
        "        timeframe=tf,\n",
        "        start=dt.datetime.fromisoformat(start_date).replace(tzinfo=tz),\n",
        "        end=dt.datetime.fromisoformat(end_date).replace(tzinfo=tz),\n",
        "    )\n",
        "\n",
        "    bars = client.get_stock_bars(req)          # SDK auto-paginates\n",
        "    df   = bars.df.sort_index()                # tidy Multi-Index (symbol, timestamp)\n",
        "\n",
        "    # If you prefer a simple DatetimeIndex because you queried one symbol:\n",
        "    if isinstance(df.index, pd.MultiIndex):\n",
        "        df = df.xs(ticker, level=\"symbol\")\n",
        "\n",
        "    return df.tz_convert(tz)                   # final consistent timezone\n",
        "import pytz\n",
        "\n",
        "\n",
        "df = get_bars_df(\n",
        "    ticker      = \"NFLX\",\n",
        "    start_date  = \"2023-01-01\",\n",
        "    end_date    = \"2025-06-28\",\n",
        "    timeframe   = \"1Min\",      # any of '5Min', '12H', '1D', etc.\n",
        "    api_key     = KEY,\n",
        "    secret_key  = SECRET,\n",
        "    tz=pytz.timezone(\"America/New_York\")\n",
        "\n",
        ")\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_rv(bars_df: pd.DataFrame,\n",
        "                          symbol: str = \"SPY\",\n",
        "                          price_col: str = \"close\",\n",
        "                          tz: str = \"America/New_York\"):\n",
        "  # ─── 1 ▸ slice desired symbol, ensure datetime index ─────────────────────\n",
        "    if isinstance(bars_df.index, pd.MultiIndex):\n",
        "        px = (bars_df.xs(symbol, level=\"symbol\")[price_col]\n",
        "                             .tz_convert(tz)\n",
        "                             .sort_index())\n",
        "    else:                                 # already single-ticker\n",
        "        px = bars_df[price_col].tz_convert(tz).sort_index()\n",
        "\n",
        "    # ─── 2 ▸ compute returns & realised vol ─────────────────────────────────\n",
        "    ret  = px.pct_change().dropna()                       # simple %\n",
        "    lret = np.log(px).diff().dropna()                     # log return\n",
        "\n",
        "    # realised vol: √(∑ intraday r²)  per *calendar* day (252-day ann. later)\n",
        "    rv_daily = (ret.pow(2)\n",
        "                   .groupby(ret.index.date).sum()\n",
        "                   .pipe(np.sqrt)\n",
        "                   .rename(\"rv\"))\n",
        "    rv_daily.index = pd.to_datetime(rv_daily.index).tz_localize(tz)\n",
        "    return rv_daily\n",
        "\n",
        "\n",
        "def plot_series(series, title, ylabel):\n",
        "    fig, ax = plt.subplots()\n",
        "    series.plot(ax=ax)\n",
        "    mu, sd = series.mean(), series.std()\n",
        "    ax.set_title(title)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.text(0.02, 0.95,\n",
        "            f\"μ = {mu:+.4f}\\nσ = {sd:.4f}\",\n",
        "            transform=ax.transAxes,\n",
        "            verticalalignment=\"top\",\n",
        "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.6))\n",
        "    plt.show()\n",
        "\n",
        "def plot_price_returns_rv(bars_df: pd.DataFrame,\n",
        "                          symbol: str = \"SPY\",\n",
        "                          price_col: str = \"close\",\n",
        "                          tz: str = \"America/New_York\"):\n",
        "    \"\"\"\n",
        "    Plot price, simple return, log return, and daily realised volatility.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bars_df   : DataFrame\n",
        "        Output of `bars.df`.  Expected MultiIndex (symbol, timestamp) or\n",
        "        a DatetimeIndex if you have already selected one ticker.\n",
        "    symbol    : str\n",
        "        Ticker to plot (ignored if bars_df is already single-ticker).\n",
        "    price_col : str\n",
        "        Column name containing prices ('close' for Alpaca bars).\n",
        "    tz        : str\n",
        "        Time-zone for date labels.\n",
        "    \"\"\"\n",
        "    # ─── 1 ▸ slice desired symbol, ensure datetime index ─────────────────────\n",
        "    if isinstance(bars_df.index, pd.MultiIndex):\n",
        "        px = (bars_df.xs(symbol, level=\"symbol\")[price_col]\n",
        "                             .tz_convert(tz)\n",
        "                             .sort_index())\n",
        "    else:                                 # already single-ticker\n",
        "        px = bars_df[price_col].tz_convert(tz).sort_index()\n",
        "\n",
        "    # ─── 2 ▸ compute returns & realised vol ─────────────────────────────────\n",
        "    ret  = px.pct_change().dropna()                       # simple %\n",
        "    lret = np.log(px).diff().dropna()                     # log return\n",
        "\n",
        "    # realised vol: √(∑ intraday r²)  per *calendar* day (252-day ann. later)\n",
        "    rv_daily = get_rv(bars_df, symbol, price_col, tz)\n",
        "\n",
        "    # ─── 3 ▸ helper to annotate μ, σ on each panel ───────────────────────────\n",
        "\n",
        "\n",
        "    # ─── 4 ▸ draw the four figures ──────────────────────────────────────────\n",
        "    plot_series(px,   f\"{symbol} price\",               \"price\")\n",
        "    plot_series(ret,  f\"{symbol} simple return rₜ\",    \"return\")\n",
        "    plot_series(lret, f\"{symbol} log return ℓₜ\",       \"log-return\")\n",
        "    plot_series(rv_daily,\n",
        "                f\"{symbol} daily realised volatility σᴿ (√∑ r²)\",\n",
        "                \"σᴿ\")\n",
        "\n",
        "plot_price_returns_rv(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WtHQyXzaiN0"
      },
      "outputs": [],
      "source": [
        "# Replace your previous get_hourly_rv with this enhanced version\n",
        "def get_hourly_rv(\n",
        "    bars_df: pd.DataFrame,\n",
        "    symbol: str = \"SPY\",\n",
        "    price_col: str = \"close\",\n",
        "    tz: str = \"America/New_York\"\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Compute hourly realized volatility during regular US trading hours,\n",
        "    labeling each hour by its _end_ time (e.g. returns in (09:00,10:00] → 10:00).\n",
        "    \"\"\"\n",
        "    # ─── 1 slice out the ticker & ensure tz-aware local index ───────────────\n",
        "    if isinstance(bars_df.index, pd.MultiIndex):\n",
        "        px = (bars_df\n",
        "              .xs(symbol, level=\"symbol\")[price_col]\n",
        "              .tz_convert(tz)\n",
        "              .sort_index())\n",
        "    else:\n",
        "        px = bars_df[price_col].tz_convert(tz).sort_index()\n",
        "\n",
        "    # ─── 2 restrict to regular session (09:30–16:00) & drop NaNs ────────────\n",
        "    ret = px.pct_change().dropna()\n",
        "\n",
        "    # ─── 3 square returns & bin into right-closed hourly buckets ────────────\n",
        "    sq = ret.pow(2)\n",
        "    # label='right' means (H-1,H] → H\n",
        "    hourly_sum = sq.groupby(pd.Grouper(freq=\"H\", label=\"right\", closed=\"right\")).sum()\n",
        "\n",
        "    # ─── 4 take sqrt to get realized vol & name it ──────────────────────────\n",
        "    rv_hourly = np.sqrt(hourly_sum).rename(\"rv_hourly\")\n",
        "\n",
        "    # ─── 5 ensure the index is tz-aware local hours ─────────────────────────\n",
        "    if rv_hourly.index.tz is None:\n",
        "        rv_hourly.index = rv_hourly.index.tz_localize(tz)\n",
        "\n",
        "    return rv_hourly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ocGa6DVakA5",
        "outputId": "725e3f0e-f405-4d9c-9f2d-b92e4774d863"
      },
      "outputs": [],
      "source": [
        "rv_hourly = get_hourly_rv(df, symbol=\"NFLX\", price_col=\"close\", tz=\"America/New_York\")\n",
        "print(rv_hourly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxjjwrSbe30n",
        "outputId": "96c374af-8a35-44c1-cdc2-cc7af3e68d7a"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "import pandas as pd\n",
        "\n",
        "def train_test_split_last_n_days(df: pd.DataFrame, n_days: int = 7):\n",
        "    \"\"\"\n",
        "    Splits a DataFrame into train and test sets,\n",
        "    where the test set is the last `n_days` calendar days of the index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df     : pd.DataFrame\n",
        "        Must have a DateTimeIndex (can be tz-aware or naive).\n",
        "    n_days : int\n",
        "        Number of days to reserve for the test set (default=7).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_df, test_df : (pd.DataFrame, pd.DataFrame)\n",
        "    \"\"\"\n",
        "    # Ensure sorted by time\n",
        "    df_sorted = df.sort_index()\n",
        "\n",
        "    # Compute cutoff timestamp\n",
        "    last_ts = df_sorted.index.max()\n",
        "    cutoff  = last_ts - pd.Timedelta(days=n_days)\n",
        "\n",
        "    # Split\n",
        "    train_df = df_sorted.loc[df_sorted.index <= cutoff]\n",
        "    test_df  = df_sorted.loc[df_sorted.index >  cutoff]\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Example usage:\n",
        "train_rv, test_set = train_test_split_last_n_days(rv_hourly, n_days=7)\n",
        "print(train_rv)\n",
        "print(f\"Train from {train_rv.index.min()} to {train_rv.index.max()} ({len(train_rv)} rows)\")\n",
        "print(f\"Test  from {test_set.index.min()} to {test_set.index.max()} ({len(test_set)} rows)\")\n",
        "training_set = train_rv.shift(-1).dropna()   # length N−1, target at t is RV at t+1\n",
        "print(training_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-rPrAose_L8",
        "outputId": "1616b7ee-7baa-46e0-d041-a262bf83db2c"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# ── Fix the 2D shape error when scaling a Series ─────────────────────────────\n",
        "\n",
        "# assume `training_set` is a pd.Series of shape (n_samples,)\n",
        "# convert to a 2D array of shape (n_samples, 1)\n",
        "train_vals = training_set.values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# now scale\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "training_scaled = scaler.fit_transform(train_vals)\n",
        "\n",
        "# ── build LSTM sequences ────────────────────────────────────────────────────\n",
        "memory = 60  # or whatever lookback window you’re using\n",
        "\n",
        "X_train, y_train = [], []\n",
        "for i in range(memory, len(training_scaled)):\n",
        "    X_train.append(training_scaled[i - memory : i, 0])\n",
        "    y_train.append(training_scaled[i, 0])\n",
        "\n",
        "# reshape into [samples, time_steps, features]\n",
        "X_train = np.array(X_train).reshape(-1, memory, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(f\"Shapes → X_train: {X_train.shape}, y_train: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "NKzhqj-6f_zJ",
        "outputId": "2c909e06-3669-4de2-a732-fafed1b35948"
      },
      "outputs": [],
      "source": [
        "# Cell 7 — Hyperparameter Tuning with Keras Tuner\n",
        "!pip install keras-tuner        # run once in your env\n",
        "\n",
        "import kerastuner as kt\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# 1) Build a hypermodel factory\n",
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    for i in range(hp.Int('n_layers', 1, 4)):\n",
        "        units    = hp.Int(f'units_{i}', min_value=8, max_value=256, step=8)\n",
        "        dropout  = hp.Float(f'dropout_{i}', 0.0, max_value=0.5, step=0.05)\n",
        "        return_seq = (i < hp.get('n_layers') - 1)\n",
        "\n",
        "        if i == 0:\n",
        "            model.add(LSTM(units, return_sequences=return_seq,\n",
        "                           input_shape=(memory,1)))\n",
        "        else:\n",
        "            model.add(LSTM(units, return_sequences=return_seq))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Dense(1))\n",
        "    model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(\n",
        "         hp.Float('lr', 1e-4, 1e-2, sampling='log')),\n",
        "      loss='mean_squared_error'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 2) Instantiate tuner\n",
        "tuner = kt.RandomSearch(\n",
        "    build_lstm_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory='lstm_tuning',\n",
        "    project_name='stock_pred'\n",
        ")\n",
        "\n",
        "# 3) Prep a simple time-series split for validation\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "# (you could also just use validation_split=0.2 in tuner.search,\n",
        "#  but this shows an explicit fold)\n",
        "\n",
        "# 4) Run the search\n",
        "tuner.search(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,   # or use validation_data=(X_val, y_val)\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)]\n",
        ")\n",
        "\n",
        "# 5) Fetch the best model & summary\n",
        "best = tuner.get_best_models(num_models=1)[0]\n",
        "best.summary()\n",
        "model = best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "VFbcQAFLerTn",
        "outputId": "ccb27ce1-72a4-42b0-d5d8-379979737104"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Cell 7 — Faster Hyperparameter Tuning with Keras Tuner (smaller search space)\n",
        "\n",
        "!pip install keras-tuner           # if not already installed\n",
        "\n",
        "import kerastuner as kt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense, Conv1D, MaxPooling1D\n",
        "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
        "\n",
        "# 1) Build a much smaller hypermodel factory\n",
        "def build_lstm_model_fast(hp):\n",
        "    model = Sequential()\n",
        "    # only 1–2 LSTM layers\n",
        "    for i in range(hp.Int('n_layers', 1, 2)):\n",
        "        units     = hp.Int(f'units_{i}', min_value=16, max_value=64, step=16)\n",
        "        dropout   = hp.Float(f'dropout_{i}', 0.0, max_value=0.3, step=0.1)\n",
        "        return_seq = (i < hp.get('n_layers') - 1)\n",
        "        if i == 0:\n",
        "            model.add(LSTM(units, return_sequences=return_seq,\n",
        "                           input_shape=(memory, 1)))\n",
        "        else:\n",
        "            model.add(LSTM(units, return_sequences=return_seq))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(\n",
        "            hp.Float('lr', 1e-3, 1e-2, sampling='log')\n",
        "        ),\n",
        "        loss='mean_squared_error'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 2) Instantiate tuner with fewer trials\n",
        "tuner_fast = kt.RandomSearch(\n",
        "    build_lstm_model_fast,\n",
        "    objective='val_loss',\n",
        "    max_trials=5,               # reduce from 20 → 5\n",
        "    executions_per_trial=1,\n",
        "    directory='lstm_tuning_fast',\n",
        "    project_name='stock_pred_fast'\n",
        ")\n",
        "\n",
        "# 3) Run the search with fewer epochs for quick feedback\n",
        "tuner_fast.search(\n",
        "    X_train, y_train,\n",
        "    epochs=10,                  # reduce from 50 → 10\n",
        "    batch_size=16,              # smaller batch\n",
        "    validation_split=0.2,\n",
        "    callbacks=[EarlyStopping(patience=3)]\n",
        ")\n",
        "\n",
        "# 4) Retrieve best model\n",
        "best_fast = tuner_fast.get_best_models(num_models=1)[0]\n",
        "best_fast.summary()\n",
        "model = best_fast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "EGAE-Xx5lYhX",
        "outputId": "33407cc1-a5d3-43d2-a6ef-5dd4065cb9d1"
      },
      "outputs": [],
      "source": [
        "# 2️⃣ Build the test‐input windows\n",
        "full_vals = rv_hourly.values.reshape(-1, 1)\n",
        "inputs   = full_vals[-len(test_set) - memory :]\n",
        "inputs_scaled = scaler.transform(inputs)\n",
        "\n",
        "X_test = []\n",
        "for i in range(memory, len(inputs_scaled)):\n",
        "    X_test.append(inputs_scaled[i - memory : i, 0])\n",
        "X_test = np.array(X_test).reshape(-1, memory, 1)\n",
        "\n",
        "# 3️⃣ Predict & invert scaling\n",
        "pred_scaled = model.predict(X_test)\n",
        "pred_rv     = scaler.inverse_transform(pred_scaled).flatten()\n",
        "\n",
        "# 4️⃣ Plot Actual vs. Predicted\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(test_set.index, test_set.values, 'o-', label='Actual RV', linewidth=2)\n",
        "plt.plot(test_set.index.shift(-1), pred_rv,       's--',label='Predicted RV', linewidth=2) # account for lag\n",
        "plt.title('Hourly Realized Volatility: Actual vs. Predicted (Last 7 Days)')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Realized Volatility')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
