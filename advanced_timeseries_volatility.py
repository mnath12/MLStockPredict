#!/usr/bin/env python
"""
Advanced Time Series Volatility Forecasting System v2.0

Implements sophisticated statistical time series methods for volatility forecasting:
1. HAR-RV with robust estimators and regime switching
2. GARCH family models (GARCH, EGARCH, GJR-GARCH, FIGARCH)
3. Stochastic Volatility models (SVR, SVJ)
4. Long memory models (ARFIMA, HAR-RV-CJ)
5. State-space models with Kalman filtering
6. Ensemble methods with dynamic model averaging
7. Rolling window forecasting framework

Usage:
    python advanced_timeseries_volatility.py AAPL --window_size 500 --forecast_horizon 5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
import warnings
from typing import Dict, List, Tuple, Optional, Union, Any
from dataclasses import dataclass
from datetime import datetime, timedelta
import sys
import argparse
import os
import time
from scipy import stats
from scipy.optimize import minimize
from scipy.special import gamma, digamma
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.kalman_filter import KalmanFilter
from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression
from arch import arch_model
from arch.univariate import GARCH, EGARCH, FIGARCH, MIDASHyperbolic
import logging

# Alpaca imports
try:
    from alpaca.data.historical import StockHistoricalDataClient
    from alpaca.data.requests import StockBarsRequest
    from alpaca.data.timeframe import TimeFrame
    from alpaca.data.enums import Adjustment
    ALPACA_AVAILABLE = True
except ImportError:
    ALPACA_AVAILABLE = False
    logger.warning("Alpaca SDK not available. Install with: pip install alpaca-py")

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ENHANCED DATA ACQUISITION WITH ALPACA ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class DataProvider:
    """Advanced data provider with multiple sources and robust error handling."""
    
    def __init__(self):
        self.alpaca_client = None
        self.initialize_alpaca()
    
    def initialize_alpaca(self):
        """Initialize Alpaca client if credentials available."""
        if not ALPACA_AVAILABLE:
            logger.info("Alpaca SDK not available, will use yfinance fallback")
            return
        
        api_key = os.getenv("APCA_API_KEY_ID")
        secret_key = os.getenv("APCA_API_SECRET_KEY")
        
        if api_key and secret_key:
            try:
                self.alpaca_client = StockHistoricalDataClient(api_key, secret_key)
                logger.info("‚úÖ Alpaca client initialized successfully")
            except Exception as e:
                logger.warning(f"Failed to initialize Alpaca client: {e}")
                self.alpaca_client = None
        else:
            logger.info("Alpaca credentials not found in environment variables")
            logger.info("Set APCA_API_KEY_ID and APCA_API_SECRET_KEY to use Alpaca")
    
    def download_prices_alpaca(self, ticker: str, start: str, end: str, tries: int = 3) -> pd.Series:
        """Download prices using Alpaca API with retry logic."""
        
        if not self.alpaca_client:
            logger.info("Alpaca client not available, falling back to yfinance")
            return self.download_prices_yfinance(ticker, start, end, tries)
        
        logger.info(f"üì° Downloading {ticker} data from Alpaca ({start} to {end})")
        
        for attempt in range(tries):
            try:
                logger.info(f"  üîÑ Alpaca attempt {attempt + 1}/{tries}...")
                
                if attempt > 0:
                    time.sleep(2 ** attempt)  # Exponential backoff
                
                # Create request
                request = StockBarsRequest(
                    symbol_or_symbols=[ticker],
                    timeframe=TimeFrame.Day,
                    start=pd.Timestamp(start, tz='America/New_York'),
                    end=pd.Timestamp(end, tz='America/New_York'),
                    adjustment=Adjustment.ALL,
                    asof=None,
                    feed=None
                )
                
                # Get data
                bars = self.alpaca_client.get_stock_bars(request)
                
                if not bars.df.empty:
                    # Process data
                    df = bars.df.droplevel(0)  # Remove symbol level from MultiIndex
                    prices = df['close'].copy()
                    prices.name = 'close'
                    
                    # Convert timezone-aware index to timezone-naive for consistency
                    if prices.index.tz is not None:
                        prices.index = prices.index.tz_convert('UTC').tz_localize(None)
                    
                    logger.info(f"  ‚úÖ Alpaca: Downloaded {len(prices)} days of data")
                    return prices.dropna()
                else:
                    raise ValueError(f"No data returned from Alpaca for {ticker}")
                
            except Exception as e:
                logger.warning(f"  ‚ùå Alpaca attempt {attempt + 1} failed: {e}")
                if attempt < tries - 1:
                    time.sleep(2 ** attempt)
                else:
                    logger.info("‚ö†Ô∏è  All Alpaca attempts failed, falling back to yfinance...")
                    return self.download_prices_yfinance(ticker, start, end, tries)
        
        return pd.Series(dtype=float)
    
    def download_prices_yfinance(self, ticker: str, start: str, end: str, tries: int = 3) -> pd.Series:
        """Download prices using yfinance with retry logic and rate limiting."""
        
        logger.info(f"üì° Downloading {ticker} data from yfinance ({start} to {end})")
        
        for attempt in range(tries):
            try:
                logger.info(f"  üîÑ yfinance attempt {attempt + 1}/{tries}...")
                
                if attempt > 0:
                    # Longer delays for yfinance to avoid rate limits
                    delay = min(60, 10 * (2 ** attempt))  # Cap at 60 seconds
                    logger.info(f"  ‚è≥ Waiting {delay} seconds to avoid rate limits...")
                    time.sleep(delay)
                
                # Download with rate limiting considerations
                data = yf.download(
                    ticker, 
                    start=start, 
                    end=end, 
                    progress=False, 
                    auto_adjust=True, 
                    threads=False,  # Single threaded to avoid rate limits
                    interval='1d',
                    prepost=False,
                    repair=True
                )
                
                if data.empty:
                    raise ValueError(f"No data returned for {ticker}")
                
                prices = data['Close'].copy()
                prices.name = 'close'
                
                logger.info(f"  ‚úÖ yfinance: Downloaded {len(prices)} days of data")
                return prices.dropna()
                
            except Exception as e:
                logger.warning(f"  ‚ùå yfinance attempt {attempt + 1} failed: {e}")
                if attempt < tries - 1:
                    # Progressive delays for rate limit handling
                    delay = min(60, 5 * (2 ** attempt))
                    time.sleep(delay)
                else:
                    raise RuntimeError(f"Failed to download data for {ticker} from all sources: {e}")
    
    def download_prices_polygon(self, ticker: str, start: str, end: str) -> pd.Series:
        """Download prices from Polygon.io (if available)."""
        # Placeholder for Polygon integration
        # You can add Polygon.io support here if you have an API key
        logger.info("Polygon.io integration not implemented")
        return pd.Series(dtype=float)
    
    def download_prices(self, ticker: str, start: str, end: str, source: str = 'auto') -> pd.Series:
        """
        Download prices with intelligent source selection.
        
        Args:
            ticker: Stock symbol
            start: Start date (YYYY-MM-DD)
            end: End date (YYYY-MM-DD)
            source: 'auto', 'alpaca', 'yfinance', 'polygon'
        """
        
        if source == 'alpaca':
            return self.download_prices_alpaca(ticker, start, end)
        elif source == 'yfinance':
            return self.download_prices_yfinance(ticker, start, end)
        elif source == 'polygon':
            return self.download_prices_polygon(ticker, start, end)
        elif source == 'auto':
            # Try sources in order of preference
            sources_to_try = []
            
            # Alpaca first if available
            if self.alpaca_client:
                sources_to_try.append('alpaca')
            
            # yfinance as fallback
            sources_to_try.append('yfinance')
            
            # Try each source
            for src in sources_to_try:
                try:
                    logger.info(f"Trying data source: {src}")
                    prices = getattr(self, f'download_prices_{src}')(ticker, start, end)
                    
                    if not prices.empty:
                        logger.info(f"‚úÖ Successfully downloaded data using {src}")
                        return prices
                    
                except Exception as e:
                    logger.warning(f"Data source {src} failed: {e}")
                    continue
            
            raise RuntimeError(f"All data sources failed for {ticker}")
        
        else:
            raise ValueError(f"Unknown data source: {source}")
    
    def validate_data_quality(self, prices: pd.Series, ticker: str) -> pd.Series:
        """Validate and clean price data."""
        
        logger.info(f"üîç Validating data quality for {ticker}")
        
        original_length = len(prices)
        
        # Remove duplicates
        prices = prices[~prices.index.duplicated(keep='first')]
        
        # Remove zero/negative prices
        prices = prices[prices > 0]
        
        # Remove extreme outliers (daily returns > 50%)
        returns = prices.pct_change().dropna()
        outlier_mask = np.abs(returns) < 0.5  # 50% daily move threshold
        
        # Keep first price + non-outlier periods
        if len(outlier_mask) > 0:
            # Create clean prices series starting with first price
            clean_indices = [prices.index[0]]  # Always keep first price
            clean_indices.extend(prices.index[1:][outlier_mask])
            clean_prices = prices.loc[clean_indices]
        else:
            clean_prices = prices
        
        # Handle missing dates by reindexing to daily frequency
        if len(clean_prices) > 1:
            # Create daily date range
            date_range = pd.date_range(
                start=clean_prices.index.min(),
                end=clean_prices.index.max(),
                freq='D'
            )
            
            # Reindex to daily frequency
            clean_prices = clean_prices.reindex(date_range)
            
            # Forward fill small gaps (up to 5 days) using fillna with limit
            clean_prices = clean_prices.fillna(method='ffill', limit=5)
            
            # Remove any remaining NaN values
            clean_prices = clean_prices.dropna()
        
        removed_points = original_length - len(clean_prices)
        if removed_points > 0:
            logger.info(f"  üßπ Cleaned data: removed {removed_points} problematic points")
        
        logger.info(f"  ‚úÖ Final dataset: {len(clean_prices)} clean data points")
        
        return clean_prices

# Create global data provider instance
data_provider = DataProvider()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ADVANCED VOLATILITY ESTIMATORS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class RobustVolatilityEstimator:
    """Advanced volatility estimators with jump-robust and microstructure-robust methods."""
    
    @staticmethod
    def realized_volatility(returns: pd.Series, annualize: bool = True) -> float:
        """Standard realized volatility."""
        rv = np.sum(returns**2)
        return np.sqrt(rv * 252) if annualize else np.sqrt(rv)
    
    @staticmethod
    def bipower_variation(returns: pd.Series, annualize: bool = True) -> float:
        """Jump-robust bipower variation estimator."""
        abs_returns = np.abs(returns)
        n = len(abs_returns)
        
        if n < 2:
            return np.nan
            
        # Bipower variation with proper scaling
        mu_1 = np.sqrt(2/np.pi)  # E[|Z|] for standard normal Z
        bv = (mu_1**(-2)) * np.sum(abs_returns[1:] * abs_returns[:-1])
        
        return np.sqrt(bv * 252) if annualize else np.sqrt(bv)
    
    @staticmethod
    def tripower_quarticity(returns: pd.Series) -> float:
        """Tripower quarticity for jump testing."""
        abs_returns = np.abs(returns)
        n = len(abs_returns)
        
        if n < 3:
            return np.nan
            
        # Tripower quarticity
        mu_43 = 2**(2/3) * gamma(7/6) / gamma(1/2)  # E[|Z|^(4/3)]
        tq = n * (mu_43**(-3)) * np.sum(
            abs_returns[2:]**(4/3) * abs_returns[1:-1]**(4/3) * abs_returns[:-2]**(4/3)
        )
        
        return tq
    
    @staticmethod
    def jump_test_bns(returns: pd.Series, significance_level: float = 0.05) -> Tuple[float, bool, float]:
        """Barndorff-Nielsen & Shephard jump test."""
        rv = RobustVolatilityEstimator.realized_volatility(returns, annualize=False)
        bv = RobustVolatilityEstimator.bipower_variation(returns, annualize=False)
        tq = RobustVolatilityEstimator.tripower_quarticity(returns)
        
        if np.isnan(bv) or np.isnan(tq) or tq <= 0:
            return 0.0, False, np.nan
        
        n = len(returns)
        
        # Test statistic
        numerator = rv - bv
        denominator = np.sqrt((2/3) * tq)
        
        if denominator == 0:
            return 0.0, False, np.nan
            
        z_stat = np.sqrt(n) * numerator / denominator
        
        # Critical value for two-sided test
        critical_value = stats.norm.ppf(1 - significance_level/2)
        has_jumps = np.abs(z_stat) > critical_value
        
        # Jump component (ensure non-negative)
        jump_component = max(0, rv - bv) if has_jumps else 0.0
        
        return jump_component, has_jumps, z_stat
    
    @staticmethod
    def microstructure_robust_rv(returns: pd.Series, q: int = 1) -> float:
        """Microstructure noise robust realized volatility using subsampling."""
        n = len(returns)
        if n < 2*q + 1:
            return RobustVolatilityEstimator.realized_volatility(returns)
        
        # Subsampled returns
        subsampled_rvs = []
        for j in range(q):
            sub_returns = returns.iloc[j::q]
            if len(sub_returns) > 1:
                sub_rv = np.sum(sub_returns**2)
                subsampled_rvs.append(sub_rv)
        
        if not subsampled_rvs:
            return RobustVolatilityEstimator.realized_volatility(returns)
        
        # Average and adjust
        avg_rv = np.mean(subsampled_rvs) * q
        return np.sqrt(avg_rv * 252)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ADVANCED HAR MODELS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AdvancedHARModel:
    """Advanced HAR models with multiple extensions."""
    
    def __init__(self, model_type: str = 'har_cj_q'):
        self.model_type = model_type
        self.model = None
        self.params = None
        self.fitted = False
        self.aic = np.inf
        self.bic = np.inf
        
    def _create_har_features(self, rv_data: pd.DataFrame) -> pd.DataFrame:
        """Create comprehensive HAR features."""
        features = pd.DataFrame(index=rv_data.index)
        
        # Basic HAR components
        features['rv_d'] = rv_data['rv']
        features['rv_w'] = rv_data['rv'].rolling(window=5, min_periods=1).mean()
        features['rv_m'] = rv_data['rv'].rolling(window=22, min_periods=1).mean()
        
        # Lagged versions
        features['rv_d_lag1'] = features['rv_d'].shift(1)
        features['rv_w_lag1'] = features['rv_w'].shift(1)
        features['rv_m_lag1'] = features['rv_m'].shift(1)
        
        if 'cj' in self.model_type.lower():
            # Continuous and Jump components
            features['c_d'] = rv_data.get('rv_continuous', features['rv_d'])
            features['c_w'] = features['c_d'].rolling(window=5, min_periods=1).mean()
            features['c_m'] = features['c_d'].rolling(window=22, min_periods=1).mean()
            
            features['j_d'] = rv_data.get('rv_jump', 0)
            features['j_w'] = features['j_d'].rolling(window=5, min_periods=1).mean()
            features['j_m'] = features['j_d'].rolling(window=22, min_periods=1).mean()
            
            # Lagged versions
            features['c_d_lag1'] = features['c_d'].shift(1)
            features['c_w_lag1'] = features['c_w'].shift(1)
            features['c_m_lag1'] = features['c_m'].shift(1)
            features['j_d_lag1'] = features['j_d'].shift(1)
            features['j_w_lag1'] = features['j_w'].shift(1)
            features['j_m_lag1'] = features['j_m'].shift(1)
        
        if 'q' in self.model_type.lower():
            # Realized quarticity (volatility of volatility)
            features['rq_d'] = rv_data.get('rv_quarticity', features['rv_d']**2)
            features['rq_w'] = features['rq_d'].rolling(window=5, min_periods=1).mean()
            features['rq_m'] = features['rq_d'].rolling(window=22, min_periods=1).mean()
            
            features['rq_d_lag1'] = features['rq_d'].shift(1)
            features['rq_w_lag1'] = features['rq_w'].shift(1)
            features['rq_m_lag1'] = features['rq_m'].shift(1)
        
        if 'leverage' in self.model_type.lower():
            # Leverage effects
            features['leverage'] = rv_data.get('leverage_proxy', 0)
            features['leverage_lag1'] = features['leverage'].shift(1)
        
        # Log transformation for some models
        if 'log' in self.model_type.lower():
            log_cols = [c for c in features.columns if c.startswith(('rv_', 'c_', 'rq_'))]
            for col in log_cols:
                features[f'log_{col}'] = np.log(features[col] + 1e-8)
        
        return features.dropna()
    
    def fit(self, rv_data: pd.DataFrame, method: str = 'mle') -> 'AdvancedHARModel':
        """Fit HAR model with various estimation methods."""
        
        # Create features
        features = self._create_har_features(rv_data)
        
        if len(features) < 50:
            raise ValueError("Insufficient data for HAR model fitting")
        
        # Target variable (next period volatility)
        y = features['rv_d'].shift(-1).dropna()
        X = features[:-1]  # Remove last row to align with y
        
        # Align indices
        common_idx = X.index.intersection(y.index)
        X = X.loc[common_idx]
        y = y.loc[common_idx]
        
        # Select model specification
        if self.model_type == 'har_basic':
            feature_cols = ['rv_d_lag1', 'rv_w_lag1', 'rv_m_lag1']
        elif self.model_type == 'har_cj':
            feature_cols = ['c_d_lag1', 'c_w_lag1', 'c_m_lag1', 'j_d_lag1', 'j_w_lag1', 'j_m_lag1']
        elif self.model_type == 'har_cj_q':
            feature_cols = ['c_d_lag1', 'c_w_lag1', 'c_m_lag1', 'j_d_lag1', 'j_w_lag1', 'j_m_lag1',
                           'rq_d_lag1', 'rq_w_lag1', 'rq_m_lag1']
        elif self.model_type == 'har_log':
            feature_cols = [c for c in X.columns if c.startswith('log_') and c.endswith('_lag1')]
        else:
            # Use all available features
            feature_cols = [c for c in X.columns if c.endswith('_lag1')]
        
        # Filter available features
        available_features = [f for f in feature_cols if f in X.columns]
        X_model = X[available_features]
        
        if X_model.empty:
            raise ValueError(f"No features available for model {self.model_type}")
        
        # Fit model
        if method == 'ols':
            X_model = sm.add_constant(X_model)
            self.model = sm.OLS(y, X_model).fit()
            self.params = self.model.params
            self.aic = self.model.aic
            self.bic = self.model.bic
            
        elif method == 'robust':
            X_model = sm.add_constant(X_model)
            self.model = sm.RLM(y, X_model).fit()
            self.params = self.model.params
            
        elif method == 'mle':
            # Maximum likelihood with normal errors
            X_model_with_const = sm.add_constant(X_model)
            
            def neg_log_likelihood(params):
                beta = params[:-1]
                sigma = params[-1]
                
                if sigma <= 0:
                    return 1e10
                
                y_pred = X_model_with_const @ beta
                residuals = y - y_pred
                
                # Normal log-likelihood
                ll = -0.5 * len(y) * np.log(2 * np.pi) - len(y) * np.log(sigma) - np.sum(residuals**2) / (2 * sigma**2)
                return -ll
            
            # Initial parameters
            ols_model = sm.OLS(y, X_model_with_const).fit()
            init_params = np.concatenate([ols_model.params, [np.sqrt(ols_model.mse_resid)]])
            
            # Optimize
            result = minimize(neg_log_likelihood, init_params, method='L-BFGS-B',
                            bounds=[(None, None)] * len(init_params[:-1]) + [(1e-6, None)])
            
            if result.success:
                self.params = pd.Series(result.x[:-1], index=X_model_with_const.columns)
                self.sigma = result.x[-1]
                self.aic = 2 * len(result.x) + 2 * result.fun
                self.bic = len(result.x) * np.log(len(y)) + 2 * result.fun
            else:
                # Fallback to OLS
                self.model = ols_model
                self.params = ols_model.params
                self.aic = ols_model.aic
                self.bic = ols_model.bic
        
        self.feature_names = available_features
        self.fitted = True
        
        return self
    
    def predict(self, rv_data: pd.DataFrame, steps_ahead: int = 1) -> np.ndarray:
        """Generate multi-step ahead forecasts."""
        if not self.fitted:
            raise ValueError("Model must be fitted before prediction")
        
        features = self._create_har_features(rv_data)
        
        if steps_ahead == 1:
            # One-step ahead forecast
            X_pred = features[self.feature_names].iloc[-1:].values.reshape(1, -1)
            
            if hasattr(self.model, 'predict'):
                # statsmodels model
                X_pred_df = pd.DataFrame(X_pred, columns=self.feature_names)
                X_pred_df = sm.add_constant(X_pred_df)
                return self.model.predict(X_pred_df).values
            else:
                # MLE model
                X_pred_with_const = np.concatenate([[1], X_pred.flatten()])
                return np.array([X_pred_with_const @ self.params])
        
        else:
            # Multi-step ahead forecast using recursive approach
            predictions = []
            current_features = features.copy()
            
            for step in range(steps_ahead):
                # One-step prediction
                pred = self.predict(current_features, steps_ahead=1)[0]
                predictions.append(pred)
                
                # Update features for next prediction
                # This is simplified - in practice you'd need to update all components
                new_row = current_features.iloc[-1:].copy()
                new_row.iloc[0, current_features.columns.get_loc('rv_d')] = pred
                
                # Update rolling averages (simplified)
                if 'rv_w' in current_features.columns:
                    recent_rv = current_features['rv_d'].iloc[-4:].tolist() + [pred]
                    new_row.iloc[0, current_features.columns.get_loc('rv_w')] = np.mean(recent_rv)
                
                if 'rv_m' in current_features.columns:
                    recent_rv = current_features['rv_d'].iloc[-21:].tolist() + [pred]
                    new_row.iloc[0, current_features.columns.get_loc('rv_m')] = np.mean(recent_rv)
                
                # Append new row
                current_features = pd.concat([current_features, new_row], ignore_index=True)
            
            return np.array(predictions)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê GARCH FAMILY MODELS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class AdvancedGARCHModels:
    """Advanced GARCH family models for volatility forecasting."""
    
    def __init__(self):
        self.models = {}
        self.fitted_models = {}
        
    def fit_garch_family(self, returns: pd.Series) -> Dict[str, Any]:
        """Fit multiple GARCH family models and select best."""
        
        results = {}
        
        # Ensure returns are clean and have enough data
        returns = returns.dropna()
        if len(returns) < 100:
            logger.warning("Insufficient data for GARCH models")
            return results
        
        # Model specifications
        garch_specs = [
            ('GARCH', {'vol': 'GARCH', 'p': 1, 'q': 1}),
            ('EGARCH', {'vol': 'EGARCH', 'p': 1, 'q': 1}),
            ('GJR-GARCH', {'vol': 'GARCH', 'p': 1, 'o': 1, 'q': 1})
        ]
        
        # Try FIGARCH only if we have lots of data
        if len(returns) > 500:
            garch_specs.append(('FIGARCH', {'vol': 'FIGARCH', 'p': 1, 'q': 1}))
        
        for model_name, spec in garch_specs:
            try:
                logger.info(f"Fitting {model_name} model...")
                
                # Create and fit model
                model = arch_model(returns * 100, **spec)  # Scale returns to percentages
                
                # Fit with robust settings
                fitted_model = model.fit(
                    disp='off', 
                    show_warning=False,
                    options={'maxiter': 1000}
                )
                
                self.fitted_models[model_name] = fitted_model
                
                # Store results
                results[model_name] = {
                    'aic': fitted_model.aic,
                    'bic': fitted_model.bic,
                    'log_likelihood': fitted_model.loglikelihood,
                    'params': fitted_model.params.to_dict(),
                    'converged': fitted_model.convergence_flag == 0
                }
                
                logger.info(f"‚úÖ {model_name}: AIC={fitted_model.aic:.2f}, BIC={fitted_model.bic:.2f}")
                
            except Exception as e:
                logger.warning(f"‚ùå Failed to fit {model_name}: {e}")
                continue
        
        # Select best model by AIC among converged models
        if results:
            converged_models = {k: v for k, v in results.items() if v.get('converged', False)}
            
            if converged_models:
                best_model = min(converged_models.keys(), key=lambda x: converged_models[x]['aic'])
                results['best_model'] = best_model
                logger.info(f"üèÜ Best GARCH model: {best_model}")
            else:
                # If no models converged, pick the one with best AIC anyway
                best_model = min(results.keys(), key=lambda x: results[x]['aic'])
                results['best_model'] = best_model
                logger.warning(f"‚ö†Ô∏è No GARCH models converged. Using best AIC: {best_model}")
        
        return results
    
    def predict_garch(self, model_name: str, horizon: int = 1) -> np.ndarray:
        """Generate GARCH volatility forecasts."""
        if model_name not in self.fitted_models:
            raise ValueError(f"Model {model_name} not fitted")
        
        fitted_model = self.fitted_models[model_name]
        forecast = fitted_model.forecast(horizon=horizon)
        
        # Return annualized volatility forecasts
        return np.sqrt(forecast.variance.values[-1] * 252)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê REGIME SWITCHING MODELS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class RegimeSwitchingVolatility:
    """Markov regime-switching volatility models."""
    
    def __init__(self, n_regimes: int = 2):
        self.n_regimes = n_regimes
        self.model = None
        self.fitted = False
        
    def fit(self, volatility_series: pd.Series) -> 'RegimeSwitchingVolatility':
        """Fit Markov regime-switching model with robust error handling."""
        
        try:
            vol_data = volatility_series.dropna()
            
            if len(vol_data) < 50:
                raise ValueError("Insufficient data for regime switching model")
            
            # Prepare data - use log volatility for better statistical properties
            log_vol = np.log(vol_data + 1e-8)
            
            # Try Markov switching model first
            try:
                logger.info(f"Fitting {self.n_regimes}-regime Markov switching model...")
                
                self.model = MarkovRegression(
                    log_vol, 
                    k_regimes=self.n_regimes, 
                    trend='c',
                    switching_trend=True,
                    switching_variance=True
                ).fit(maxiter=500, em_iter=20)
                
                self.fitted = True
                logger.info("‚úÖ Regime switching model fitted successfully")
                
            except Exception as e:
                logger.warning(f"Markov switching failed: {e}. Trying simpler approach...")
                
                # Fallback to threshold autoregressive model
                try:
                    # Simple 2-regime threshold model
                    vol_median = vol_data.median()
                    
                    # Low volatility regime
                    low_vol_mask = vol_data <= vol_median
                    high_vol_mask = vol_data > vol_median
                    
                    if low_vol_mask.sum() > 10 and high_vol_mask.sum() > 10:
                        # Fit separate AR models for each regime
                        from statsmodels.tsa.ar_model import AutoReg
                        
                        low_vol_data = log_vol[low_vol_mask]
                        high_vol_data = log_vol[high_vol_mask]
                        
                        self.low_regime_model = AutoReg(low_vol_data, lags=1, trend='c').fit()
                        self.high_regime_model = AutoReg(high_vol_data, lags=1, trend='c').fit()
                        
                        self.threshold = vol_median
                        self.model_type = 'threshold'
                        self.fitted = True
                        
                        logger.info("‚úÖ Threshold regime model fitted as fallback")
                    else:
                        raise ValueError("Insufficient data in regimes")
                        
                except Exception as e2:
                    logger.warning(f"Threshold model also failed: {e2}. Using simple AR fallback...")
                    
                    # Final fallback to simple AR model
                    from statsmodels.tsa.ar_model import AutoReg
                    self.model = AutoReg(log_vol, lags=1, trend='c').fit()
                    self.model_type = 'ar_fallback'
                    self.fitted = True
                    
                    logger.info("‚úÖ Simple AR model fitted as final fallback")
                    
        except Exception as e:
            logger.warning(f"All regime switching approaches failed: {e}")
            self.fitted = False
        
        return self
    
    def predict(self, steps_ahead: int = 1) -> Tuple[np.ndarray, np.ndarray]:
        """Generate regime-switching forecasts with probabilities."""
        if not self.fitted:
            raise ValueError("Model must be fitted before prediction")
        
        try:
            if hasattr(self, 'model_type'):
                
                if self.model_type == 'threshold':
                    # Use threshold model
                    # For simplicity, assume we stay in current regime
                    last_vol = np.exp(self.model.endog[-1]) if hasattr(self.model, 'endog') else self.threshold
                    
                    if last_vol <= self.threshold:
                        # Low volatility regime
                        forecast = self.low_regime_model.forecast(steps_ahead)
                        regime_probs = np.array([0.8, 0.2])  # 80% prob of staying in low regime
                    else:
                        # High volatility regime
                        forecast = self.high_regime_model.forecast(steps_ahead)
                        regime_probs = np.array([0.2, 0.8])  # 80% prob of staying in high regime
                    
                    predictions = np.exp(forecast)  # Transform back from log space
                    
                elif self.model_type == 'ar_fallback':
                    # Simple AR forecast
                    forecast = self.model.forecast(steps_ahead)
                    predictions = np.exp(forecast)
                    regime_probs = np.array([1.0])  # Single regime
                    
                else:
                    # This shouldn't happen, but handle gracefully
                    predictions = np.array([self.threshold] * steps_ahead)
                    regime_probs = np.array([0.5, 0.5])
                    
            else:
                # Original Markov switching model
                forecast = self.model.forecast(steps_ahead)
                predictions = np.exp(forecast)  # Transform back from log
                
                # Get regime probabilities
                if hasattr(self.model, 'smoothed_marginal_probabilities'):
                    regime_probs = self.model.smoothed_marginal_probabilities.iloc[-1].values
                else:
                    regime_probs = np.array([0.5, 0.5])  # Default equal probabilities
            
            return predictions, regime_probs
            
        except Exception as e:
            logger.warning(f"Regime switching prediction failed: {e}")
            # Return simple persistence forecast
            last_vol = 20.0  # Default volatility level
            return np.array([last_vol] * steps_ahead), np.array([1.0])

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê STATE SPACE MODELS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class StateSpaceVolatility:
    """Simplified state-space volatility model."""
    
    def __init__(self, n_states: int = 1):
        self.n_states = n_states
        self.fitted = False
        self.params = None
        self.last_state = None
        
    def fit(self, volatility_series: pd.Series) -> 'StateSpaceVolatility':
        """Fit state-space model using simple AR approach as fallback."""
        
        try:
            # Try to fit a simple AR(1) model as state-space approximation
            # This is more robust than full Kalman filter implementation
            
            vol_data = volatility_series.dropna()
            if len(vol_data) < 10:
                raise ValueError("Insufficient data for state-space model")
            
            # Log transform for better properties
            log_vol = np.log(vol_data + 1e-8)
            
            # Fit AR(1) model
            from statsmodels.tsa.ar_model import AutoReg
            
            try:
                ar_model = AutoReg(log_vol, lags=1, trend='c').fit()
                
                self.params = {
                    'const': ar_model.params[0],
                    'ar1': ar_model.params[1],
                    'sigma': np.sqrt(ar_model.sigma2)
                }
                
                self.last_state = log_vol.iloc[-1]
                self.fitted = True
                
                logger.info("Fitted state-space model (AR approximation)")
                
            except Exception as e:
                # Final fallback to simple persistence
                logger.warning(f"AR model failed: {e}. Using persistence model.")
                
                self.params = {
                    'const': 0,
                    'ar1': 0.95,  # High persistence
                    'sigma': log_vol.std()
                }
                
                self.last_state = log_vol.iloc[-1]
                self.fitted = True
                
        except Exception as e:
            logger.warning(f"State-space fitting error: {e}")
            # Set as unfitted to skip in ensemble
            self.fitted = False
        
        return self
    
    def predict(self, steps_ahead: int = 1) -> np.ndarray:
        """Generate state-space forecasts."""
        if not self.fitted:
            raise ValueError("Model must be fitted before prediction")
        
        try:
            forecasts = []
            current_state = self.last_state
            
            for step in range(steps_ahead):
                # AR(1) prediction in log space
                next_state = self.params['const'] + self.params['ar1'] * current_state
                
                # Transform back to volatility space
                vol_forecast = np.exp(next_state)
                forecasts.append(vol_forecast)
                
                # Update state for next prediction
                current_state = next_state
            
            return np.array(forecasts)
            
        except Exception as e:
            logger.warning(f"State-space prediction error: {e}")
            # Return last observed volatility as fallback
            return np.array([np.exp(self.last_state)] * steps_ahead)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ENSEMBLE METHODS WITH DYNAMIC MODEL AVERAGING ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class DynamicModelAveraging:
    """Dynamic model averaging for volatility forecasting."""
    
    def __init__(self, forgetting_factor: float = 0.95):
        self.forgetting_factor = forgetting_factor
        self.models = {}
        self.weights = {}
        self.performance_history = {}
        
    def add_model(self, name: str, model: Any):
        """Add a model to the ensemble."""
        self.models[name] = model
        self.weights[name] = 1.0 / len(self.models)  # Equal initial weights
        self.performance_history[name] = []
    
    def update_weights(self, forecasts: Dict[str, float], actual: float):
        """Update model weights based on recent performance."""
        
        # Calculate forecast errors
        errors = {}
        for name, forecast in forecasts.items():
            error = (forecast - actual) ** 2
            errors[name] = error
            self.performance_history[name].append(error)
        
        # Calculate exponentially weighted average errors
        weighted_errors = {}
        for name in self.models.keys():
            if len(self.performance_history[name]) > 0:
                weights = np.array([self.forgetting_factor ** i for i in range(len(self.performance_history[name]))])
                weights = weights[::-1]  # Most recent gets highest weight
                
                weighted_error = np.average(self.performance_history[name], weights=weights)
                weighted_errors[name] = weighted_error
            else:
                weighted_errors[name] = 1.0
        
        # Update weights (inverse of weighted errors)
        total_inv_error = sum(1.0 / max(error, 1e-8) for error in weighted_errors.values())
        
        for name in self.models.keys():
            self.weights[name] = (1.0 / max(weighted_errors[name], 1e-8)) / total_inv_error
    
    def ensemble_forecast(self, data: Any, horizon: int = 1) -> Tuple[float, Dict[str, float]]:
        """Generate ensemble forecast."""
        
        individual_forecasts = {}
        
        for name, model in self.models.items():
            try:
                if hasattr(model, 'predict'):
                    forecast = model.predict(data, steps_ahead=horizon)
                    if isinstance(forecast, np.ndarray):
                        forecast = forecast[0] if len(forecast) > 0 else 0
                    individual_forecasts[name] = forecast
                else:
                    individual_forecasts[name] = 0
            except Exception as e:
                logger.warning(f"Model {name} prediction failed: {e}")
                individual_forecasts[name] = 0
        
        # Weighted average
        ensemble_forecast = sum(
            self.weights[name] * forecast 
            for name, forecast in individual_forecasts.items()
        )
        
        return ensemble_forecast, individual_forecasts

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ROLLING WINDOW FORECASTING FRAMEWORK ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class ForecastResult:
    """Container for forecast results."""
    date: pd.Timestamp
    actual_price: float
    actual_volatility: float
    forecasted_volatility: float
    model_forecasts: Dict[str, float]
    model_weights: Dict[str, float]
    confidence_interval: Tuple[float, float]

class RollingWindowForecaster:
    """Sophisticated rolling window forecasting system."""
    
    def __init__(
        self, 
        window_size: int = 252, 
        forecast_horizon: int = 5,
        refit_frequency: int = 10,
        confidence_level: float = 0.95
    ):
        self.window_size = window_size
        self.forecast_horizon = forecast_horizon
        self.refit_frequency = refit_frequency
        self.confidence_level = confidence_level
        
        # Initialize models
        self.har_models = {
            'HAR_Basic': AdvancedHARModel('har_basic'),
            'HAR_CJ': AdvancedHARModel('har_cj'),
            'HAR_CJ_Q': AdvancedHARModel('har_cj_q'),
            'HAR_Log': AdvancedHARModel('har_log')
        }
        
        self.garch_models = AdvancedGARCHModels()
        self.regime_model = RegimeSwitchingVolatility(n_regimes=2)
        self.state_space_model = StateSpaceVolatility(n_states=1)
        self.ensemble = DynamicModelAveraging(forgetting_factor=0.95)
        
        # Results storage
        self.results = []
        self.performance_metrics = {}
        self.fitted_windows = 0
        
    def calculate_volatility_data(self, prices: pd.Series) -> pd.DataFrame:
        """Calculate comprehensive volatility measures."""
        
        # Daily returns
        returns = np.log(prices / prices.shift(1)).dropna()
        
        # Rolling volatility calculations
        volatility_data = []
        
        for i in range(1, len(returns)):
            date = returns.index[i]
            
            # Use expanding window for initial periods, then rolling
            if i < 22:
                window_returns = returns.iloc[:i+1]
            else:
                window_returns = returns.iloc[max(0, i-21):i+1]
            
            # Standard realized volatility
            rv = RobustVolatilityEstimator.realized_volatility(window_returns)
            
            # Jump-robust measures
            rv_bv = RobustVolatilityEstimator.bipower_variation(window_returns)
            jump_comp, has_jumps, jump_stat = RobustVolatilityEstimator.jump_test_bns(window_returns)
            
            # Microstructure-robust RV
            rv_robust = RobustVolatilityEstimator.microstructure_robust_rv(window_returns)
            
            # Quarticity (simplified)
            rq = np.sum(window_returns**4) * 252  # Annualized realized quarticity
            
            # Leverage proxy (return-volatility correlation)
            if len(window_returns) > 5:
                leverage = np.corrcoef(window_returns[:-1], window_returns[1:]**2)[0, 1]
            else:
                leverage = 0
            
            volatility_data.append({
                'date': date,
                'rv': rv,
                'rv_continuous': rv_bv if not np.isnan(rv_bv) else rv,
                'rv_jump': jump_comp * np.sqrt(252),  # Annualized
                'rv_robust': rv_robust,
                'rv_quarticity': rq,
                'has_jumps': has_jumps,
                'jump_stat': jump_stat,
                'leverage_proxy': leverage if not np.isnan(leverage) else 0,
                'returns': window_returns.iloc[-1] if len(window_returns) > 0 else 0
            })
        
        return pd.DataFrame(volatility_data).set_index('date')
    
    def fit_models(self, volatility_data: pd.DataFrame, returns: pd.Series):
        """Fit all models on current window."""
        
        try:
            # Fit HAR models
            for name, model in self.har_models.items():
                try:
                    model.fit(volatility_data)
                    self.ensemble.add_model(name, model)
                    logger.info(f"Fitted {name}")
                except Exception as e:
                    logger.warning(f"Failed to fit {name}: {e}")
            
            # Fit GARCH models
            garch_results = self.garch_models.fit_garch_family(returns)
            if 'best_model' in garch_results:
                self.ensemble.add_model('GARCH_Best', self.garch_models)
            
            # Fit regime switching model
            try:
                self.regime_model.fit(volatility_data['rv'])
                self.ensemble.add_model('Regime_Switching', self.regime_model)
                logger.info("Fitted regime switching model")
            except Exception as e:
                logger.warning(f"Failed to fit regime switching model: {e}")
            
            # Fit state-space model
            try:
                self.state_space_model.fit(volatility_data['rv'])
                self.ensemble.add_model('State_Space', self.state_space_model)
                logger.info("Fitted state-space model")
            except Exception as e:
                logger.warning(f"Failed to fit state-space model: {e}")
            
            self.fitted_windows += 1
            
        except Exception as e:
            logger.error(f"Model fitting error: {e}")
    
    def generate_forecast(self, volatility_data: pd.DataFrame, returns: pd.Series) -> Tuple[float, Dict[str, float], Dict[str, float]]:
        """Generate ensemble forecast."""
        
        # Get individual model forecasts
        forecasts = {}
        
        # HAR models
        for name, model in self.har_models.items():
            if model.fitted:
                try:
                    pred = model.predict(volatility_data, steps_ahead=1)
                    forecasts[name] = pred[0] if isinstance(pred, np.ndarray) else pred
                except Exception as e:
                    logger.warning(f"{name} prediction failed: {e}")
        
        # GARCH models
        if hasattr(self.garch_models, 'fitted_models') and self.garch_models.fitted_models:
            try:
                best_garch = list(self.garch_models.fitted_models.keys())[0]
                garch_pred = self.garch_models.predict_garch(best_garch, horizon=1)
                forecasts['GARCH_Best'] = garch_pred[0] if isinstance(garch_pred, np.ndarray) else garch_pred
            except Exception as e:
                logger.warning(f"GARCH prediction failed: {e}")
        
        # Regime switching
        if self.regime_model.fitted:
            try:
                regime_pred, regime_probs = self.regime_model.predict(steps_ahead=1)
                forecasts['Regime_Switching'] = regime_pred[0] if isinstance(regime_pred, np.ndarray) else regime_pred
            except Exception as e:
                logger.warning(f"Regime switching prediction failed: {e}")
        
        # State-space
        if self.state_space_model.fitted:
            try:
                ss_pred = self.state_space_model.predict(steps_ahead=1)
                forecasts['State_Space'] = ss_pred[0] if isinstance(ss_pred, np.ndarray) else ss_pred
            except Exception as e:
                logger.warning(f"State-space prediction failed: {e}")
        
        # Ensemble forecast
        if forecasts:
            ensemble_forecast, individual_forecasts = self.ensemble.ensemble_forecast(volatility_data, horizon=1)
            weights = self.ensemble.weights.copy()
        else:
            # Fallback to simple average
            last_vol = volatility_data['rv'].iloc[-1]
            ensemble_forecast = last_vol
            individual_forecasts = {'Fallback': last_vol}
            weights = {'Fallback': 1.0}
        
        return ensemble_forecast, individual_forecasts, weights
    
    def calculate_confidence_interval(self, forecasts: Dict[str, float]) -> Tuple[float, float]:
        """Calculate confidence interval from individual forecasts."""
        
        if len(forecasts) < 2:
            # Use simple heuristic if only one forecast
            base_forecast = list(forecasts.values())[0]
            margin = 0.2 * base_forecast  # 20% margin
            return (base_forecast - margin, base_forecast + margin)
        
        forecast_values = list(forecasts.values())
        mean_forecast = np.mean(forecast_values)
        std_forecast = np.std(forecast_values)
        
        # Use t-distribution for small samples
        if len(forecast_values) < 30:
            from scipy.stats import t
            df = len(forecast_values) - 1
            alpha = 1 - self.confidence_level
            t_critical = t.ppf(1 - alpha/2, df)
            margin = t_critical * std_forecast
        else:
            # Normal distribution for large samples
            z_critical = stats.norm.ppf(1 - (1 - self.confidence_level)/2)
            margin = z_critical * std_forecast
        
        return (mean_forecast - margin, mean_forecast + margin)
    
    def run_rolling_forecast(self, prices: pd.Series) -> List[ForecastResult]:
        """Run complete rolling window forecasting."""
        
        logger.info(f"Starting rolling forecast with window_size={self.window_size}, horizon={self.forecast_horizon}")
        
        # Calculate volatility data
        volatility_data = self.calculate_volatility_data(prices)
        returns = np.log(prices / prices.shift(1)).dropna()
        
        # Align data
        common_dates = volatility_data.index.intersection(returns.index)
        volatility_data = volatility_data.loc[common_dates]
        returns = returns.loc[common_dates]
        
        total_periods = len(volatility_data)
        
        if total_periods < self.window_size + self.forecast_horizon:
            raise ValueError(f"Insufficient data: need at least {self.window_size + self.forecast_horizon} periods")
        
        logger.info(f"Total periods: {total_periods}, forecast periods: {total_periods - self.window_size}")
        
        # Rolling forecast loop
        for i in range(self.window_size, total_periods - self.forecast_horizon + 1):
            current_date = volatility_data.index[i + self.forecast_horizon - 1]
            
            # Extract current window
            window_vol_data = volatility_data.iloc[i - self.window_size:i]
            window_returns = returns.iloc[i - self.window_size:i]
            
            # Refit models periodically
            if (i - self.window_size) % self.refit_frequency == 0 or self.fitted_windows == 0:
                logger.info(f"Refitting models at period {i}")
                self.fit_models(window_vol_data, window_returns)
            
            # Generate forecast
            try:
                ensemble_forecast, individual_forecasts, weights = self.generate_forecast(
                    window_vol_data, window_returns
                )
                
                # Calculate confidence interval
                conf_interval = self.calculate_confidence_interval(individual_forecasts)
                
                # Get actual values
                actual_price = prices.iloc[i + self.forecast_horizon - 1]
                actual_vol = volatility_data.iloc[i + self.forecast_horizon - 1]['rv']
                
                # Update ensemble weights with actual performance
                if len(self.results) > 0:  # Not first prediction
                    last_forecasts = self.results[-1].model_forecasts
                    last_actual = self.results[-1].actual_volatility
                    self.ensemble.update_weights(last_forecasts, last_actual)
                
                # Store results
                result = ForecastResult(
                    date=current_date,
                    actual_price=actual_price,
                    actual_volatility=actual_vol,
                    forecasted_volatility=ensemble_forecast,
                    model_forecasts=individual_forecasts,
                    model_weights=weights,
                    confidence_interval=conf_interval
                )
                
                self.results.append(result)
                
                # Log progress
                if (i - self.window_size) % 20 == 0:
                    error = abs(ensemble_forecast - actual_vol)
                    logger.info(f"Period {i}: Forecast={ensemble_forecast:.2f}%, Actual={actual_vol:.2f}%, Error={error:.2f}%")
                
            except Exception as e:
                logger.error(f"Forecast generation failed at period {i}: {e}")
                continue
        
        logger.info(f"Completed rolling forecast: {len(self.results)} predictions generated")
        return self.results
    
    def calculate_performance_metrics(self) -> Dict[str, Any]:
        """Calculate comprehensive performance metrics."""
        
        if not self.results:
            return {}
        
        # Extract arrays
        actual_vols = np.array([r.actual_volatility for r in self.results])
        forecast_vols = np.array([r.forecasted_volatility for r in self.results])
        
        # Basic metrics
        mae = mean_absolute_error(actual_vols, forecast_vols)
        mse = mean_squared_error(actual_vols, forecast_vols)
        rmse = np.sqrt(mse)
        mape = np.mean(np.abs((actual_vols - forecast_vols) / actual_vols)) * 100
        r2 = r2_score(actual_vols, forecast_vols)
        
        # Directional accuracy
        actual_changes = np.diff(actual_vols)
        forecast_changes = np.diff(forecast_vols)
        directional_accuracy = np.mean(np.sign(actual_changes) == np.sign(forecast_changes)) * 100
        
        # Confidence interval coverage
        in_ci = 0
        for result in self.results:
            if result.confidence_interval[0] <= result.actual_volatility <= result.confidence_interval[1]:
                in_ci += 1
        ci_coverage = (in_ci / len(self.results)) * 100
        
        # Model performance breakdown
        model_performance = {}
        for model_name in self.ensemble.models.keys():
            model_forecasts = []
            model_actuals = []
            
            for result in self.results:
                if model_name in result.model_forecasts:
                    model_forecasts.append(result.model_forecasts[model_name])
                    model_actuals.append(result.actual_volatility)
            
            if model_forecasts:
                model_rmse = np.sqrt(mean_squared_error(model_actuals, model_forecasts))
                model_performance[model_name] = {
                    'rmse': model_rmse,
                    'avg_weight': np.mean([r.model_weights.get(model_name, 0) for r in self.results])
                }
        
        # Regime analysis
        vol_regimes = pd.cut(actual_vols, bins=3, labels=['Low', 'Medium', 'High'])
        regime_performance = {}
        
        for regime in ['Low', 'Medium', 'High']:
            regime_mask = vol_regimes == regime
            if regime_mask.sum() > 0:
                regime_rmse = np.sqrt(mean_squared_error(
                    actual_vols[regime_mask], 
                    forecast_vols[regime_mask]
                ))
                regime_performance[regime] = {
                    'rmse': regime_rmse,
                    'count': regime_mask.sum()
                }
        
        self.performance_metrics = {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'mape': mape,
            'r2': r2,
            'directional_accuracy': directional_accuracy,
            'ci_coverage': ci_coverage,
            'model_performance': model_performance,
            'regime_performance': regime_performance,
            'n_predictions': len(self.results)
        }
        
        return self.performance_metrics

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê VISUALIZATION AND ANALYSIS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def plot_comprehensive_results(forecaster: RollingWindowForecaster, ticker: str, prices: pd.Series):
    """Create comprehensive visualization of forecasting results."""
    
    if not forecaster.results:
        logger.error("No results to plot")
        return
    
    # Extract data for plotting
    dates = [r.date for r in forecaster.results]
    actual_vols = [r.actual_volatility for r in forecaster.results]
    forecast_vols = [r.forecasted_volatility for r in forecaster.results]
    actual_prices = [r.actual_price for r in forecaster.results]
    ci_lower = [r.confidence_interval[0] for r in forecaster.results]
    ci_upper = [r.confidence_interval[1] for r in forecaster.results]
    
    # Create figure with subplots
    fig, axes = plt.subplots(3, 2, figsize=(20, 15))
    fig.suptitle(f'{ticker} - Advanced Time Series Volatility Forecasting Results', fontsize=16, fontweight='bold')
    
    # 1. Price series (top left)
    ax1 = axes[0, 0]
    forecast_start_idx = len(prices) - len(dates)
    in_sample_prices = prices.iloc[:forecast_start_idx]
    forecast_prices = prices.iloc[forecast_start_idx:]
    
    ax1.plot(in_sample_prices.index, in_sample_prices.values, 'b-', label='In-sample', linewidth=1)
    ax1.plot(forecast_prices.index, forecast_prices.values, 'r-', label='Out-of-sample', linewidth=2)
    ax1.set_title('Price Series: In-sample vs Out-of-sample')
    ax1.set_ylabel('Price ($)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Volatility forecasts vs actual (top right)
    ax2 = axes[0, 1]
    ax2.plot(dates, actual_vols, 'b-', label='Actual Volatility', linewidth=2)
    ax2.plot(dates, forecast_vols, 'r--', label='Ensemble Forecast', linewidth=2)
    ax2.fill_between(dates, ci_lower, ci_upper, alpha=0.3, color='red', label=f'{forecaster.confidence_level*100:.0f}% CI')
    ax2.set_title('Volatility Forecasts vs Actual')
    ax2.set_ylabel('Volatility (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Forecast errors (middle left)
    ax3 = axes[1, 0]
    errors = np.array(forecast_vols) - np.array(actual_vols)
    error_pct = (errors / np.array(actual_vols)) * 100
    
    ax3.plot(dates, error_pct, 'g-', alpha=0.7, linewidth=1)
    ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax3.axhline(y=np.mean(error_pct), color='red', linestyle='--', alpha=0.7, 
                label=f'Mean Error: {np.mean(error_pct):.1f}%')
    ax3.set_title('Forecast Errors Over Time')
    ax3.set_ylabel('Error (%)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. Model weights evolution (middle right)
    ax4 = axes[1, 1]
    model_names = list(forecaster.ensemble.weights.keys())
    if model_names:
        weights_over_time = {name: [] for name in model_names}
        
        for result in forecaster.results:
            for name in model_names:
                weights_over_time[name].append(result.model_weights.get(name, 0))
        
        for name, weights in weights_over_time.items():
            if any(w > 0 for w in weights):  # Only plot if model has non-zero weights
                ax4.plot(dates, weights, label=name, linewidth=2)
        
        ax4.set_title('Dynamic Model Weights')
        ax4.set_ylabel('Weight')
        ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax4.grid(True, alpha=0.3)
    
    # 5. Error distribution (bottom left)
    ax5 = axes[2, 0]
    ax5.hist(error_pct, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    ax5.axvline(x=0, color='red', linestyle='--', linewidth=2)
    ax5.axvline(x=np.mean(error_pct), color='orange', linestyle='--', linewidth=2, 
                label=f'Mean: {np.mean(error_pct):.1f}%')
    ax5.axvline(x=np.median(error_pct), color='green', linestyle='--', linewidth=2, 
                label=f'Median: {np.median(error_pct):.1f}%')
    ax5.set_title('Forecast Error Distribution')
    ax5.set_xlabel('Error (%)')
    ax5.set_ylabel('Frequency')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. Performance metrics summary (bottom right)
    ax6 = axes[2, 1]
    ax6.axis('off')
    
    metrics = forecaster.performance_metrics
    if metrics:
        metrics_text = f"""
        Performance Summary:
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        RMSE: {metrics['rmse']:.3f}%
        MAE: {metrics['mae']:.3f}%
        MAPE: {metrics['mape']:.1f}%
        R¬≤: {metrics['r2']:.3f}
        Directional Accuracy: {metrics['directional_accuracy']:.1f}%
        CI Coverage: {metrics['ci_coverage']:.1f}%
        
        Top Models by Weight:
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        """
        
        # Add top 3 models by average weight
        if 'model_performance' in metrics:
            sorted_models = sorted(
                metrics['model_performance'].items(),
                key=lambda x: x[1]['avg_weight'],
                reverse=True
            )[:3]
            
            for i, (model, perf) in enumerate(sorted_models, 1):
                metrics_text += f"{i}. {model}: {perf['avg_weight']:.1%} weight\n"
                metrics_text += f"   RMSE: {perf['rmse']:.3f}%\n"
        
        ax6.text(0.05, 0.95, metrics_text, transform=ax6.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    plt.tight_layout()
    plt.show()
    
    # Print detailed results
    print_detailed_results(forecaster, ticker)

def print_detailed_results(forecaster: RollingWindowForecaster, ticker: str):
    """Print comprehensive results analysis."""
    
    metrics = forecaster.performance_metrics
    
    print(f"\n{'‚ïê'*80}")
    print(f"üéØ ADVANCED TIME SERIES VOLATILITY FORECASTING RESULTS: {ticker}")
    print(f"{'‚ïê'*80}")
    
    print(f"\nForecast Configuration:")
    print(f"{'‚îÄ'*40}")
    print(f"Window Size: {forecaster.window_size} periods")
    print(f"Forecast Horizon: {forecaster.forecast_horizon} periods")
    print(f"Refit Frequency: {forecaster.refit_frequency} periods")
    print(f"Total Predictions: {metrics['n_predictions']}")
    print(f"Models Fitted: {forecaster.fitted_windows} times")
    
    print(f"\nOverall Performance:")
    print(f"{'‚îÄ'*40}")
    print(f"RMSE: {metrics['rmse']:.3f}%")
    print(f"MAE: {metrics['mae']:.3f}%")
    print(f"MAPE: {metrics['mape']:.1f}%")
    print(f"R¬≤: {metrics['r2']:.3f}")
    print(f"Directional Accuracy: {metrics['directional_accuracy']:.1f}%")
    print(f"Confidence Interval Coverage: {metrics['ci_coverage']:.1f}%")
    
    # Performance assessment
    if metrics['r2'] > 0.3 and metrics['directional_accuracy'] > 60:
        print(f"üìà Excellent forecasting performance!")
    elif metrics['r2'] > 0.1 and metrics['directional_accuracy'] > 55:
        print(f"üìä Good forecasting performance")
    elif metrics['directional_accuracy'] > 50:
        print(f"‚ö†Ô∏è  Moderate performance - better than random")
    else:
        print(f"‚ùå Poor performance - needs improvement")
    
    print(f"\nModel Performance Breakdown:")
    print(f"{'‚îÄ'*40}")
    if 'model_performance' in metrics:
        sorted_models = sorted(
            metrics['model_performance'].items(),
            key=lambda x: x[1]['avg_weight'],
            reverse=True
        )
        
        for model, perf in sorted_models:
            print(f"{model:<20}: RMSE={perf['rmse']:.3f}%, Avg Weight={perf['avg_weight']:.1%}")
    
    print(f"\nVolatility Regime Analysis:")
    print(f"{'‚îÄ'*40}")
    if 'regime_performance' in metrics:
        for regime, perf in metrics['regime_performance'].items():
            print(f"{regime} Vol Regime: RMSE={perf['rmse']:.3f}% ({perf['count']} periods)")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê MAIN EXECUTION ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    """Main execution function with enhanced data handling."""
    
    parser = argparse.ArgumentParser(description='Advanced Time Series Volatility Forecasting')
    parser.add_argument('ticker', nargs='?', default='AAPL', help='Stock ticker symbol')
    parser.add_argument('--window_size', type=int, default=252, help='Rolling window size')
    parser.add_argument('--forecast_horizon', type=int, default=5, help='Forecast horizon')
    parser.add_argument('--refit_frequency', type=int, default=10, help='Model refit frequency')
    parser.add_argument('--start_date', type=str, default=None, help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end_date', type=str, default=None, help='End date (YYYY-MM-DD)')
    parser.add_argument('--data_source', type=str, default='auto', 
                       choices=['auto', 'alpaca', 'yfinance', 'polygon'],
                       help='Data source preference')
    parser.add_argument('--confidence_level', type=float, default=0.95, help='Confidence level for intervals')
    
    args = parser.parse_args()
    
    # Set default dates if not provided
    if args.end_date is None:
        args.end_date = datetime.now().strftime('%Y-%m-%d')
    
    if args.start_date is None:
        # Need enough data for window + forecasts + buffer
        needed_days = args.window_size + args.forecast_horizon + 365
        start_dt = datetime.now() - timedelta(days=needed_days)
        args.start_date = start_dt.strftime('%Y-%m-%d')
    
    print(f"üöÄ Advanced Time Series Volatility Forecasting for {args.ticker}")
    print(f"üìÖ Data period: {args.start_date} to {args.end_date}")
    print(f"üîß Window size: {args.window_size}, Forecast horizon: {args.forecast_horizon}")
    print(f"üì° Data source preference: {args.data_source}")
    
    try:
        # Download data using enhanced data provider
        logger.info("Downloading price data with enhanced provider...")
        
        # Add buffer for volatility calculations
        buffer_start = (datetime.strptime(args.start_date, '%Y-%m-%d') - timedelta(days=100)).strftime('%Y-%m-%d')
        
        prices = data_provider.download_prices(
            ticker=args.ticker,
            start=buffer_start,
            end=args.end_date,
            source=args.data_source
        )
        
        if prices.empty:
            raise ValueError(f"No data available for {args.ticker}")
        
        # Validate and clean data
        prices = data_provider.validate_data_quality(prices, args.ticker)
        
        # Trim to requested start date
        prices = prices[prices.index >= args.start_date]
        
        logger.info(f"üìà Final dataset: {len(prices)} days ({prices.index[0].date()} to {prices.index[-1].date()})")
        
        # Check if we have enough data
        min_required = args.window_size + args.forecast_horizon + 50  # Extra buffer
        if len(prices) < min_required:
            raise ValueError(f"Insufficient data: need at least {min_required} days, got {len(prices)}")
        
        # Initialize forecaster with enhanced configuration
        forecaster = RollingWindowForecaster(
            window_size=args.window_size,
            forecast_horizon=args.forecast_horizon,
            refit_frequency=args.refit_frequency,
            confidence_level=args.confidence_level
        )
        
        # Add data source information to forecaster for reporting
        forecaster.data_source = args.data_source
        forecaster.data_quality_score = calculate_data_quality_score(prices)
        
        # Run rolling forecast
        logger.info("üîÑ Starting rolling window forecasting...")
        results = forecaster.run_rolling_forecast(prices)
        
        if not results:
            raise ValueError("No forecasts generated")
        
        logger.info(f"‚úÖ Generated {len(results)} forecasts")
        
        # Calculate performance metrics
        performance = forecaster.calculate_performance_metrics()
        
        # Enhanced results reporting
        print_enhanced_summary(forecaster, args.ticker, performance)
        
        # Create visualizations
        plot_comprehensive_results(forecaster, args.ticker, prices)
        
        # Save results with enhanced metadata
        results_df = create_enhanced_results_dataframe(results, forecaster, args)
        
        filename = f"{args.ticker}_advanced_volatility_forecast_{args.start_date}_{args.end_date}.csv"
        results_df.to_csv(filename, index=False)
        logger.info(f"üíæ Results saved to {filename}")
        
        # Save model configuration and performance summary
        save_model_summary(forecaster, args, performance, filename.replace('.csv', '_summary.json'))
        
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

def calculate_data_quality_score(prices: pd.Series) -> float:
    """Calculate a data quality score (0-100)."""
    
    score = 100.0
    
    # Check for missing data
    total_days = (prices.index[-1] - prices.index[0]).days
    actual_days = len(prices)
    completeness = actual_days / total_days
    score *= completeness
    
    # Check for extreme moves (penalize poor data quality)
    returns = prices.pct_change().dropna()
    extreme_moves = (np.abs(returns) > 0.2).sum()  # >20% daily moves
    if len(returns) > 0:
        extreme_ratio = extreme_moves / len(returns)
        score *= (1 - min(extreme_ratio * 10, 0.5))  # Max 50% penalty
    
    # Check for consecutive identical values (suspicious)
    consecutive_identical = (prices.diff() == 0).sum()
    if len(prices) > 0:
        identical_ratio = consecutive_identical / len(prices)
        score *= (1 - min(identical_ratio * 5, 0.3))  # Max 30% penalty
    
    return max(score, 0.0)

def print_enhanced_summary(forecaster: RollingWindowForecaster, ticker: str, metrics: Dict[str, Any]):
    """Print enhanced results summary including data source information."""
    
    print(f"\n{'‚ïê'*90}")
    print(f"üéØ ADVANCED TIME SERIES VOLATILITY FORECASTING RESULTS: {ticker}")
    print(f"{'‚ïê'*90}")
    
    # Data source and quality information
    print(f"\nData Information:")
    print(f"{'‚îÄ'*50}")
    print(f"Data Source: {getattr(forecaster, 'data_source', 'Unknown')}")
    if hasattr(data_provider, 'alpaca_client') and data_provider.alpaca_client:
        print(f"Alpaca Status: ‚úÖ Connected")
    else:
        print(f"Alpaca Status: ‚ùå Not available (using fallback)")
    
    quality_score = getattr(forecaster, 'data_quality_score', 0)
    print(f"Data Quality Score: {quality_score:.1f}/100")
    
    if quality_score >= 90:
        print(f"Data Quality: üü¢ Excellent")
    elif quality_score >= 75:
        print(f"Data Quality: üü° Good")
    elif quality_score >= 60:
        print(f"Data Quality: üü† Fair")
    else:
        print(f"Data Quality: üî¥ Poor - results may be unreliable")
    
    # Continue with original summary
    print(f"\nForecast Configuration:")
    print(f"{'‚îÄ'*50}")
    print(f"Window Size: {forecaster.window_size} periods")
    print(f"Forecast Horizon: {forecaster.forecast_horizon} periods")
    print(f"Refit Frequency: {forecaster.refit_frequency} periods")
    print(f"Total Predictions: {metrics['n_predictions']}")
    print(f"Models Fitted: {forecaster.fitted_windows} times")
    print(f"Confidence Level: {forecaster.confidence_level*100:.0f}%")
    
    print(f"\nOverall Performance:")
    print(f"{'‚îÄ'*50}")
    print(f"RMSE: {metrics['rmse']:.3f}%")
    print(f"MAE: {metrics['mae']:.3f}%")
    print(f"MAPE: {metrics['mape']:.1f}%")
    print(f"R¬≤: {metrics['r2']:.3f}")
    print(f"Directional Accuracy: {metrics['directional_accuracy']:.1f}%")
    print(f"Confidence Interval Coverage: {metrics['ci_coverage']:.1f}%")
    
    # Enhanced performance assessment
    performance_score = calculate_performance_score(metrics)
    print(f"Overall Performance Score: {performance_score:.1f}/100")
    
    if performance_score >= 80:
        print(f"üìà Excellent forecasting performance! Suitable for professional trading.")
    elif performance_score >= 65:
        print(f"üìä Good forecasting performance. Useful for risk management.")
    elif performance_score >= 50:
        print(f"‚ö†Ô∏è  Moderate performance. Better than random but limited economic value.")
    else:
        print(f"‚ùå Poor performance. Model needs significant improvement.")
    
    # Model performance breakdown
    print(f"\nModel Performance Breakdown:")
    print(f"{'‚îÄ'*50}")
    if 'model_performance' in metrics:
        sorted_models = sorted(
            metrics['model_performance'].items(),
            key=lambda x: x[1]['avg_weight'],
            reverse=True
        )
        
        for model, perf in sorted_models:
            weight_indicator = "ü•á" if perf['avg_weight'] > 0.3 else "ü•à" if perf['avg_weight'] > 0.15 else "ü•â"
            print(f"{weight_indicator} {model:<20}: RMSE={perf['rmse']:.3f}%, Avg Weight={perf['avg_weight']:.1%}")
    
    # Volatility regime analysis
    print(f"\nVolatility Regime Analysis:")
    print(f"{'‚îÄ'*50}")
    if 'regime_performance' in metrics:
        for regime, perf in metrics['regime_performance'].items():
            regime_emoji = "üü¢" if regime == "Low" else "üü°" if regime == "Medium" else "üî¥"
            print(f"{regime_emoji} {regime} Vol Regime: RMSE={perf['rmse']:.3f}% ({perf['count']} periods)")

def calculate_performance_score(metrics: Dict[str, Any]) -> float:
    """Calculate overall performance score (0-100)."""
    
    score = 0.0
    
    # R¬≤ component (40% weight)
    r2 = max(0, min(metrics.get('r2', 0), 1))  # Clamp to [0,1]
    score += r2 * 40
    
    # Directional accuracy component (30% weight)
    dir_acc = metrics.get('directional_accuracy', 50) / 100
    dir_score = max(0, (dir_acc - 0.5) * 2)  # Convert 50-100% to 0-1 scale
    score += dir_score * 30
    
    # MAPE component (20% weight) - lower is better
    mape = metrics.get('mape', 100)
    mape_score = max(0, 1 - mape / 50)  # 0% MAPE = 1, 50% MAPE = 0
    score += mape_score * 20
    
    # CI coverage component (10% weight)
    ci_coverage = metrics.get('ci_coverage', 0) / 100
    # Penalize both under and over coverage (ideal is around 95%)
    ci_score = 1 - abs(ci_coverage - 0.95) / 0.95
    score += max(0, ci_score) * 10
    
    return max(0, min(score, 100))

def create_enhanced_results_dataframe(results: List[ForecastResult], forecaster: RollingWindowForecaster, args) -> pd.DataFrame:
    """Create enhanced results DataFrame with additional metadata."""
    
    results_data = []
    
    for r in results:
        row = {
            'date': r.date,
            'actual_price': r.actual_price,
            'actual_volatility': r.actual_volatility,
            'forecasted_volatility': r.forecasted_volatility,
            'forecast_error': r.actual_volatility - r.forecasted_volatility,
            'forecast_error_pct': ((r.actual_volatility - r.forecasted_volatility) / r.actual_volatility) * 100,
            'ci_lower': r.confidence_interval[0],
            'ci_upper': r.confidence_interval[1],
            'ci_width': r.confidence_interval[1] - r.confidence_interval[0],
            'in_ci': r.confidence_interval[0] <= r.actual_volatility <= r.confidence_interval[1]
        }
        
        # Add individual model forecasts
        for model_name, forecast in r.model_forecasts.items():
            row[f'forecast_{model_name.lower()}'] = forecast
        
        # Add model weights
        for model_name, weight in r.model_weights.items():
            row[f'weight_{model_name.lower()}'] = weight
        
        results_data.append(row)
    
    df = pd.DataFrame(results_data)
    
    # Add metadata columns
    df['ticker'] = args.ticker
    df['window_size'] = args.window_size
    df['forecast_horizon'] = args.forecast_horizon
    df['data_source'] = args.data_source
    df['data_quality_score'] = getattr(forecaster, 'data_quality_score', 0)
    
    return df

def save_model_summary(forecaster: RollingWindowForecaster, args, performance: Dict[str, Any], filename: str):
    """Save model configuration and performance summary to JSON."""
    
    import json
    
    summary = {
        'configuration': {
            'ticker': args.ticker,
            'window_size': args.window_size,
            'forecast_horizon': args.forecast_horizon,
            'refit_frequency': args.refit_frequency,
            'confidence_level': args.confidence_level,
            'data_source': args.data_source,
            'start_date': args.start_date,
            'end_date': args.end_date
        },
        'data_quality': {
            'data_quality_score': getattr(forecaster, 'data_quality_score', 0),
            'alpaca_available': ALPACA_AVAILABLE and data_provider.alpaca_client is not None
        },
        'performance': performance,
        'performance_score': calculate_performance_score(performance),
        'model_info': {
            'total_models': len(forecaster.ensemble.models),
            'fitted_windows': forecaster.fitted_windows,
            'predictions_generated': len(forecaster.results)
        },
        'timestamp': datetime.now().isoformat()
    }
    
    with open(filename, 'w') as f:
        json.dump(summary, f, indent=2, default=str)
    
    logger.info(f"üìã Model summary saved to {filename}")

if __name__ == "__main__":
    main()